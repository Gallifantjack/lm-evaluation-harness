usage: huggingface-cli <command> [<args>] login [-h] [--token TOKEN]
                                                [--add-to-git-credential]
huggingface-cli <command> [<args>] login: error: argument --token: expected one argument
Running evaluation for model CohereForAI/aya-23-35B
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-06-14:07:29:54,424 INFO     [utils.py:145] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-06-14:07:29:54,424 INFO     [utils.py:148] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-06-14:07:29:54,424 INFO     [utils.py:161] NumExpr defaulting to 8 threads.
2024-06-14:07:29:54,652 INFO     [config.py:58] PyTorch version 2.3.0 available.
2024-06-14:07:30:08,855 INFO     [__main__.py:156] Verbosity set to INFO
2024-06-14:07:30:13,137 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-06-14:07:30:36,428 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-06-14:07:30:36,431 INFO     [__main__.py:229] Selected Tasks: ['b4bqa']
2024-06-14:07:30:36,464 WARNING  [other.py:349] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Downloading shards:   0%|          | 0/15 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 15/15 [00:00<00:00, 1136.18it/s]
2024-06-14:07:30:37,090 INFO     [modeling.py:989] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:04<01:05,  4.69s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:09<00:59,  4.57s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:13<00:54,  4.52s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:18<00:49,  4.50s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:22<00:45,  4.51s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:27<00:40,  4.51s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:31<00:36,  4.50s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:36<00:31,  4.50s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:40<00:27,  4.50s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:45<00:22,  4.50s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:49<00:18,  4.56s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:54<00:13,  4.53s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:58<00:09,  4.51s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [01:03<00:04,  4.50s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:04<00:00,  3.49s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:04<00:00,  4.29s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-14:07:31:43,285 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-14:07:31:43,285 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-14:07:31:43,310 INFO     [task.py:343] Building contexts for task on rank 0...
2024-06-14:07:31:43,371 INFO     [evaluator.py:314] Running loglikelihood requests
  0%|          | 0/4192 [00:00<?, ?it/s]  0%|          | 1/4192 [00:07<9:01:25,  7.75s/it]  2%|▏         | 65/4192 [00:09<07:08,  9.64it/s]   3%|▎         | 129/4192 [00:10<03:40, 18.45it/s]  5%|▍         | 193/4192 [00:11<02:32, 26.15it/s]  6%|▌         | 257/4192 [00:12<02:00, 32.54it/s]  8%|▊         | 321/4192 [00:13<01:42, 37.67it/s]  9%|▉         | 385/4192 [00:15<01:31, 41.65it/s] 11%|█         | 449/4192 [00:16<01:23, 44.63it/s] 12%|█▏        | 513/4192 [00:17<01:18, 46.79it/s] 14%|█▍        | 577/4192 [00:18<01:14, 48.36it/s] 15%|█▌        | 641/4192 [00:20<01:11, 49.84it/s] 17%|█▋        | 705/4192 [00:21<01:08, 50.87it/s] 18%|█▊        | 769/4192 [00:22<01:06, 51.60it/s] 20%|█▉        | 833/4192 [00:23<01:04, 52.13it/s] 21%|██▏       | 897/4192 [00:24<01:02, 52.50it/s] 23%|██▎       | 961/4192 [00:26<01:01, 52.72it/s] 24%|██▍       | 1025/4192 [00:27<00:59, 52.88it/s] 26%|██▌       | 1089/4192 [00:28<00:58, 53.01it/s] 28%|██▊       | 1153/4192 [00:29<00:57, 53.06it/s] 29%|██▉       | 1217/4192 [00:30<00:55, 53.18it/s] 31%|███       | 1281/4192 [00:32<00:54, 53.26it/s] 32%|███▏      | 1345/4192 [00:33<00:53, 53.30it/s] 34%|███▎      | 1409/4192 [00:34<00:52, 53.33it/s] 35%|███▌      | 1473/4192 [00:35<00:50, 53.38it/s] 37%|███▋      | 1537/4192 [00:36<00:49, 53.39it/s] 38%|███▊      | 1601/4192 [00:38<00:48, 53.36it/s] 40%|███▉      | 1665/4192 [00:39<00:47, 53.34it/s] 41%|████      | 1729/4192 [00:40<00:46, 53.35it/s] 43%|████▎     | 1793/4192 [00:41<00:44, 53.32it/s] 44%|████▍     | 1857/4192 [00:42<00:43, 53.54it/s] 46%|████▌     | 1921/4192 [00:43<00:42, 53.68it/s] 47%|████▋     | 1985/4192 [00:45<00:41, 53.75it/s] 49%|████▉     | 2049/4192 [00:46<00:39, 53.81it/s] 50%|█████     | 2113/4192 [00:47<00:38, 53.86it/s] 52%|█████▏    | 2177/4192 [00:48<00:37, 53.88it/s] 53%|█████▎    | 2241/4192 [00:49<00:36, 53.88it/s] 55%|█████▍    | 2305/4192 [00:51<00:35, 53.89it/s] 57%|█████▋    | 2369/4192 [00:52<00:33, 53.87it/s] 58%|█████▊    | 2433/4192 [00:53<00:32, 53.87it/s] 60%|█████▉    | 2497/4192 [00:54<00:31, 53.86it/s] 61%|██████    | 2561/4192 [00:55<00:30, 53.93it/s] 63%|██████▎   | 2625/4192 [00:57<00:29, 53.98it/s] 64%|██████▍   | 2689/4192 [00:58<00:27, 54.02it/s] 66%|██████▌   | 2753/4192 [00:59<00:26, 54.06it/s] 67%|██████▋   | 2817/4192 [01:00<00:25, 54.05it/s] 69%|██████▊   | 2881/4192 [01:01<00:24, 54.07it/s] 70%|███████   | 2945/4192 [01:02<00:23, 54.06it/s] 72%|███████▏  | 3009/4192 [01:04<00:21, 54.06it/s] 73%|███████▎  | 3073/4192 [01:05<00:20, 54.09it/s] 75%|███████▍  | 3137/4192 [01:06<00:19, 54.09it/s] 76%|███████▋  | 3201/4192 [01:07<00:18, 54.09it/s] 78%|███████▊  | 3265/4192 [01:08<00:16, 55.14it/s] 79%|███████▉  | 3329/4192 [01:09<00:15, 55.89it/s] 81%|████████  | 3393/4192 [01:11<00:14, 56.44it/s] 82%|████████▏ | 3457/4192 [01:12<00:12, 56.84it/s] 84%|████████▍ | 3521/4192 [01:13<00:11, 57.09it/s] 86%|████████▌ | 3585/4192 [01:14<00:10, 57.31it/s] 87%|████████▋ | 3649/4192 [01:15<00:09, 57.47it/s] 89%|████████▊ | 3713/4192 [01:16<00:08, 57.57it/s] 90%|█████████ | 3777/4192 [01:17<00:07, 57.64it/s] 92%|█████████▏| 3841/4192 [01:18<00:06, 57.69it/s] 93%|█████████▎| 3905/4192 [01:19<00:04, 57.84it/s] 95%|█████████▍| 3969/4192 [01:20<00:03, 57.91it/s] 96%|█████████▌| 4033/4192 [01:22<00:02, 57.97it/s] 98%|█████████▊| 4097/4192 [01:23<00:01, 58.50it/s] 99%|█████████▉| 4161/4192 [01:23<00:00, 66.96it/s]100%|██████████| 4192/4192 [01:23<00:00, 50.03it/s]
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 64
hf (pretrained=CohereForAI/aya-23-35B,parallelize=True,load_in_4bit=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)
|Tasks|Version|Filter|n-shot| Metric |Value|   |Stderr|
|-----|-------|------|-----:|--------|----:|---|-----:|
|b4bqa|Yaml   |none  |     0|acc     |0.958|±  |0.0062|
|     |       |none  |     0|acc_norm|0.958|±  |0.0062|

All evaluations completed.
