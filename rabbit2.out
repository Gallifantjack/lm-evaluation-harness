usage: huggingface-cli <command> [<args>] login [-h] [--token TOKEN]
                                                [--add-to-git-credential]
huggingface-cli <command> [<args>] login: error: argument --token: expected one argument
Running evaluation for model meta-llama/Llama-2-7b-hf
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-06-13:21:45:57,055 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-06-13:21:45:57,055 INFO     [utils.py:161] NumExpr defaulting to 8 threads.
2024-06-13:21:45:57,320 INFO     [config.py:58] PyTorch version 2.3.0 available.
2024-06-13:21:46:07,276 INFO     [__main__.py:156] Verbosity set to INFO
2024-06-13:21:46:12,258 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-06-13:21:46:39,016 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-06-13:21:46:39,020 INFO     [__main__.py:229] Selected Tasks: ['b4bqa']
2024-06-13:21:46:39,082 WARNING  [other.py:349] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:  50%|█████     | 1/2 [03:24<03:24, 204.26s/it]Downloading shards: 100%|██████████| 2/2 [04:34<00:00, 125.18s/it]Downloading shards: 100%|██████████| 2/2 [04:34<00:00, 137.04s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:22<00:22, 22.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 13.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.91s/it]
2024-06-13:21:51:46,544 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-13:21:51:46,545 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-13:21:51:46,579 INFO     [task.py:343] Building contexts for task on rank 0...
2024-06-13:21:51:46,643 INFO     [evaluator.py:314] Running loglikelihood requests
  0%|          | 0/4192 [00:00<?, ?it/s]  0%|          | 1/4192 [00:04<4:46:47,  4.11s/it]  2%|▏         | 65/4192 [00:04<03:37, 18.99it/s]   3%|▎         | 129/4192 [00:05<01:48, 37.53it/s]  5%|▍         | 193/4192 [00:05<01:12, 55.06it/s]  6%|▌         | 257/4192 [00:06<00:55, 70.62it/s]  8%|▊         | 321/4192 [00:06<00:46, 83.61it/s]  9%|▉         | 385/4192 [00:07<00:40, 94.30it/s] 11%|█         | 449/4192 [00:07<00:36, 102.59it/s] 12%|█▏        | 513/4192 [00:08<00:33, 108.77it/s] 14%|█▍        | 577/4192 [00:08<00:31, 113.43it/s] 15%|█▌        | 641/4192 [00:09<00:30, 117.14it/s] 17%|█▋        | 705/4192 [00:09<00:29, 119.85it/s] 18%|█▊        | 769/4192 [00:10<00:28, 121.82it/s] 20%|█▉        | 833/4192 [00:10<00:27, 123.23it/s] 21%|██▏       | 897/4192 [00:11<00:26, 124.15it/s] 23%|██▎       | 961/4192 [00:11<00:25, 124.83it/s] 24%|██▍       | 1025/4192 [00:12<00:25, 125.49it/s] 26%|██▌       | 1089/4192 [00:12<00:24, 125.89it/s] 28%|██▊       | 1153/4192 [00:13<00:24, 126.34it/s] 29%|██▉       | 1217/4192 [00:13<00:23, 126.60it/s] 31%|███       | 1281/4192 [00:14<00:22, 126.71it/s] 32%|███▏      | 1345/4192 [00:14<00:22, 126.78it/s] 34%|███▎      | 1409/4192 [00:15<00:21, 126.85it/s] 35%|███▌      | 1473/4192 [00:15<00:21, 126.86it/s] 37%|███▋      | 1537/4192 [00:16<00:20, 129.07it/s] 38%|███▊      | 1601/4192 [00:16<00:19, 130.72it/s] 40%|███▉      | 1665/4192 [00:17<00:19, 131.90it/s] 41%|████      | 1729/4192 [00:17<00:18, 132.77it/s] 43%|████▎     | 1793/4192 [00:18<00:17, 133.30it/s] 44%|████▍     | 1857/4192 [00:18<00:17, 133.66it/s] 46%|████▌     | 1921/4192 [00:19<00:16, 133.91it/s] 47%|████▋     | 1985/4192 [00:19<00:16, 134.04it/s] 49%|████▉     | 2049/4192 [00:20<00:15, 134.45it/s] 50%|█████     | 2113/4192 [00:20<00:15, 134.75it/s] 52%|█████▏    | 2177/4192 [00:21<00:14, 134.96it/s] 53%|█████▎    | 2241/4192 [00:21<00:14, 135.02it/s] 55%|█████▍    | 2305/4192 [00:21<00:13, 135.08it/s] 57%|█████▋    | 2369/4192 [00:22<00:13, 135.11it/s] 58%|█████▊    | 2433/4192 [00:22<00:13, 135.18it/s] 60%|█████▉    | 2497/4192 [00:23<00:12, 135.19it/s] 61%|██████    | 2561/4192 [00:23<00:12, 135.23it/s] 63%|██████▎   | 2625/4192 [00:24<00:11, 135.17it/s] 64%|██████▍   | 2689/4192 [00:24<00:11, 135.62it/s] 66%|██████▌   | 2753/4192 [00:25<00:10, 135.90it/s] 67%|██████▋   | 2817/4192 [00:25<00:10, 136.13it/s] 69%|██████▊   | 2881/4192 [00:26<00:09, 136.29it/s] 70%|███████   | 2945/4192 [00:26<00:09, 136.38it/s] 72%|███████▏  | 3009/4192 [00:27<00:08, 136.48it/s] 73%|███████▎  | 3073/4192 [00:27<00:08, 136.55it/s] 75%|███████▍  | 3137/4192 [00:28<00:07, 136.60it/s] 76%|███████▋  | 3201/4192 [00:28<00:07, 136.67it/s] 78%|███████▊  | 3265/4192 [00:29<00:06, 136.63it/s] 79%|███████▉  | 3329/4192 [00:29<00:06, 136.92it/s] 81%|████████  | 3393/4192 [00:29<00:05, 137.16it/s] 82%|████████▏ | 3457/4192 [00:30<00:05, 137.37it/s] 84%|████████▍ | 3521/4192 [00:30<00:04, 137.50it/s] 86%|████████▌ | 3585/4192 [00:31<00:04, 137.46it/s] 87%|████████▋ | 3649/4192 [00:31<00:03, 137.41it/s] 89%|████████▊ | 3713/4192 [00:32<00:03, 140.09it/s] 90%|█████████ | 3777/4192 [00:32<00:02, 142.05it/s] 92%|█████████▏| 3841/4192 [00:33<00:02, 143.40it/s] 93%|█████████▎| 3905/4192 [00:33<00:01, 144.42it/s] 95%|█████████▍| 3969/4192 [00:34<00:01, 145.16it/s] 96%|█████████▌| 4033/4192 [00:34<00:01, 145.96it/s] 98%|█████████▊| 4097/4192 [00:34<00:00, 146.51it/s] 99%|█████████▉| 4161/4192 [00:35<00:00, 169.99it/s]100%|██████████| 4192/4192 [00:35<00:00, 119.39it/s]
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 64
hf (pretrained=meta-llama/Llama-2-7b-hf,parallelize=True,load_in_4bit=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)
|Tasks|Version|Filter|n-shot| Metric |Value |   |Stderr|
|-----|-------|------|-----:|--------|-----:|---|-----:|
|b4bqa|Yaml   |none  |     0|acc     |0.6861|±  |0.0143|
|     |       |none  |     0|acc_norm|0.6861|±  |0.0143|

Running evaluation for model meta-llama/Meta-Llama-3-8B
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-06-13:21:52:29,135 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-06-13:21:52:29,135 INFO     [utils.py:161] NumExpr defaulting to 8 threads.
2024-06-13:21:52:29,367 INFO     [config.py:58] PyTorch version 2.3.0 available.
2024-06-13:21:52:38,936 INFO     [__main__.py:156] Verbosity set to INFO
2024-06-13:21:52:43,758 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-06-13:21:53:11,288 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-06-13:21:53:11,290 INFO     [__main__.py:229] Selected Tasks: ['b4bqa']
2024-06-13:21:53:11,366 WARNING  [other.py:349] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:  25%|██▌       | 1/4 [00:19<00:59, 19.99s/it]Downloading shards:  50%|█████     | 2/4 [00:45<00:46, 23.32s/it]Downloading shards:  75%|███████▌  | 3/4 [01:11<00:24, 24.62s/it]Downloading shards: 100%|██████████| 4/4 [01:20<00:00, 18.17s/it]Downloading shards: 100%|██████████| 4/4 [01:20<00:00, 20.02s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:27,  9.22s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:19<00:19,  9.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:28<00:09,  9.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  6.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.75s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-13:21:55:06,377 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-13:21:55:06,377 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-13:21:55:06,409 INFO     [task.py:343] Building contexts for task on rank 0...
2024-06-13:21:55:06,472 INFO     [evaluator.py:314] Running loglikelihood requests
  0%|          | 0/4192 [00:00<?, ?it/s]  0%|          | 1/4192 [00:03<4:35:12,  3.94s/it]  2%|▏         | 65/4192 [00:04<03:31, 19.50it/s]   3%|▎         | 129/4192 [00:05<01:46, 37.97it/s]  5%|▍         | 193/4192 [00:05<01:13, 54.65it/s]  6%|▌         | 257/4192 [00:06<00:56, 69.44it/s]  8%|▊         | 321/4192 [00:06<00:47, 81.66it/s]  9%|▉         | 385/4192 [00:07<00:41, 91.35it/s] 11%|█         | 449/4192 [00:07<00:37, 98.78it/s] 12%|█▏        | 513/4192 [00:08<00:35, 104.53it/s] 14%|█▍        | 577/4192 [00:08<00:33, 108.81it/s] 15%|█▌        | 641/4192 [00:09<00:31, 111.81it/s] 17%|█▋        | 705/4192 [00:09<00:30, 114.04it/s] 18%|█▊        | 769/4192 [00:10<00:29, 115.58it/s] 20%|█▉        | 833/4192 [00:10<00:28, 116.64it/s] 21%|██▏       | 897/4192 [00:11<00:27, 118.45it/s] 23%|██▎       | 961/4192 [00:12<00:26, 119.71it/s] 24%|██▍       | 1025/4192 [00:12<00:26, 120.60it/s] 26%|██▌       | 1089/4192 [00:13<00:25, 121.23it/s] 28%|██▊       | 1153/4192 [00:13<00:24, 121.66it/s] 29%|██▉       | 1217/4192 [00:14<00:24, 121.96it/s] 31%|███       | 1281/4192 [00:14<00:23, 122.15it/s] 32%|███▏      | 1345/4192 [00:15<00:23, 122.21it/s] 34%|███▎      | 1409/4192 [00:15<00:22, 122.21it/s] 35%|███▌      | 1473/4192 [00:16<00:22, 122.57it/s] 37%|███▋      | 1537/4192 [00:16<00:21, 122.86it/s] 38%|███▊      | 1601/4192 [00:17<00:21, 123.14it/s] 40%|███▉      | 1665/4192 [00:17<00:20, 123.18it/s] 41%|████      | 1729/4192 [00:18<00:19, 123.19it/s] 43%|████▎     | 1793/4192 [00:18<00:19, 123.37it/s] 44%|████▍     | 1857/4192 [00:19<00:18, 123.40it/s] 46%|████▌     | 1921/4192 [00:19<00:18, 123.38it/s] 47%|████▋     | 1985/4192 [00:20<00:17, 123.27it/s] 49%|████▉     | 2049/4192 [00:20<00:17, 125.53it/s] 50%|█████     | 2113/4192 [00:21<00:16, 127.14it/s] 52%|█████▏    | 2177/4192 [00:21<00:15, 128.31it/s] 53%|█████▎    | 2241/4192 [00:22<00:15, 129.13it/s] 55%|█████▍    | 2305/4192 [00:22<00:14, 129.71it/s] 57%|█████▋    | 2369/4192 [00:23<00:14, 130.12it/s] 58%|█████▊    | 2433/4192 [00:23<00:13, 130.39it/s] 60%|█████▉    | 2497/4192 [00:24<00:12, 130.58it/s] 61%|██████    | 2561/4192 [00:24<00:12, 130.69it/s] 63%|██████▎   | 2625/4192 [00:25<00:11, 130.79it/s] 64%|██████▍   | 2689/4192 [00:25<00:11, 130.78it/s] 66%|██████▌   | 2753/4192 [00:26<00:10, 131.15it/s] 67%|██████▋   | 2817/4192 [00:26<00:10, 131.37it/s] 69%|██████▊   | 2881/4192 [00:27<00:09, 131.54it/s] 70%|███████   | 2945/4192 [00:27<00:09, 131.65it/s] 72%|███████▏  | 3009/4192 [00:28<00:08, 131.69it/s] 73%|███████▎  | 3073/4192 [00:28<00:08, 131.71it/s] 75%|███████▍  | 3137/4192 [00:29<00:08, 131.83it/s] 76%|███████▋  | 3201/4192 [00:29<00:07, 131.86it/s] 78%|███████▊  | 3265/4192 [00:30<00:07, 131.86it/s] 79%|███████▉  | 3329/4192 [00:30<00:06, 131.79it/s] 81%|████████  | 3393/4192 [00:31<00:06, 131.77it/s] 82%|████████▏ | 3457/4192 [00:31<00:05, 132.28it/s] 84%|████████▍ | 3521/4192 [00:32<00:05, 132.65it/s] 86%|████████▌ | 3585/4192 [00:32<00:04, 132.90it/s] 87%|████████▋ | 3649/4192 [00:32<00:04, 133.12it/s] 89%|████████▊ | 3713/4192 [00:33<00:03, 133.36it/s] 90%|█████████ | 3777/4192 [00:33<00:03, 133.44it/s] 92%|█████████▏| 3841/4192 [00:34<00:02, 133.39it/s] 93%|█████████▎| 3905/4192 [00:34<00:02, 133.75it/s] 95%|█████████▍| 3969/4192 [00:35<00:01, 133.97it/s] 96%|█████████▌| 4033/4192 [00:35<00:01, 134.14it/s] 98%|█████████▊| 4097/4192 [00:36<00:00, 135.67it/s] 99%|█████████▉| 4161/4192 [00:36<00:00, 157.38it/s]100%|██████████| 4192/4192 [00:36<00:00, 114.66it/s]
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 64
hf (pretrained=meta-llama/Meta-Llama-3-8B,parallelize=True,load_in_4bit=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)
|Tasks|Version|Filter|n-shot| Metric |Value |   |Stderr|
|-----|-------|------|-----:|--------|-----:|---|-----:|
|b4bqa|Yaml   |none  |     0|acc     |0.9618|±  |0.0059|
|     |       |none  |     0|acc_norm|0.9618|±  |0.0059|

Running evaluation for model mistralai/Mixtral-8x7B-v0.1
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-06-13:21:55:50,141 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-06-13:21:55:50,141 INFO     [utils.py:161] NumExpr defaulting to 8 threads.
2024-06-13:21:55:50,385 INFO     [config.py:58] PyTorch version 2.3.0 available.
2024-06-13:21:55:59,976 INFO     [__main__.py:156] Verbosity set to INFO
2024-06-13:21:56:04,831 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-06-13:21:56:30,720 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-06-13:21:56:30,723 INFO     [__main__.py:229] Selected Tasks: ['b4bqa']
2024-06-13:21:56:30,784 WARNING  [other.py:349] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]Downloading shards:   5%|▌         | 1/19 [00:20<06:11, 20.65s/it]Downloading shards:  11%|█         | 2/19 [00:52<07:42, 27.21s/it]Downloading shards:  16%|█▌        | 3/19 [01:17<06:57, 26.10s/it]Downloading shards:  21%|██        | 4/19 [01:44<06:39, 26.61s/it]Downloading shards:  26%|██▋       | 5/19 [02:13<06:26, 27.60s/it]Downloading shards:  32%|███▏      | 6/19 [02:44<06:12, 28.68s/it]Downloading shards:  37%|███▋      | 7/19 [03:14<05:48, 29.03s/it]Downloading shards:  42%|████▏     | 8/19 [03:43<05:20, 29.15s/it]Downloading shards:  47%|████▋     | 9/19 [04:13<04:51, 29.15s/it]Downloading shards:  53%|█████▎    | 10/19 [04:55<04:59, 33.22s/it]Downloading shards:  58%|█████▊    | 11/19 [05:24<04:14, 31.83s/it]Downloading shards:  63%|██████▎   | 12/19 [05:51<03:32, 30.39s/it]Downloading shards:  68%|██████▊   | 13/19 [06:17<02:55, 29.22s/it]Downloading shards:  74%|███████▎  | 14/19 [06:42<02:19, 27.83s/it]Downloading shards:  79%|███████▉  | 15/19 [07:08<01:49, 27.39s/it]Downloading shards:  84%|████████▍ | 16/19 [07:32<01:19, 26.41s/it]Downloading shards:  89%|████████▉ | 17/19 [07:58<00:52, 26.09s/it]Downloading shards:  95%|█████████▍| 18/19 [08:26<00:26, 26.73s/it]Downloading shards: 100%|██████████| 19/19 [08:55<00:00, 27.35s/it]Downloading shards: 100%|██████████| 19/19 [08:55<00:00, 28.17s/it]
Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:09<02:44,  9.11s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:18<02:34,  9.10s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:26<02:22,  8.91s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:41<02:44, 10.99s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:49<02:22, 10.21s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:58<02:07,  9.83s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [01:07<01:54,  9.55s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [01:16<01:42,  9.34s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [01:25<01:32,  9.21s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [01:34<01:21,  9.09s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [01:43<01:12,  9.10s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [01:52<01:02,  8.96s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [02:01<00:54,  9.05s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [02:10<00:45,  9.08s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [02:19<00:36,  9.07s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [02:28<00:27,  9.09s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [02:38<00:18,  9.25s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [02:47<00:09,  9.18s/it]Loading checkpoint shards: 100%|██████████| 19/19 [02:55<00:00,  8.85s/it]Loading checkpoint shards: 100%|██████████| 19/19 [02:55<00:00,  9.25s/it]
2024-06-13:22:08:28,937 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-13:22:08:28,937 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-13:22:08:28,972 INFO     [task.py:343] Building contexts for task on rank 0...
2024-06-13:22:08:29,037 INFO     [evaluator.py:314] Running loglikelihood requests
  0%|          | 0/4192 [00:00<?, ?it/s]  0%|          | 1/4192 [00:07<8:43:52,  7.50s/it]  2%|▏         | 65/4192 [00:08<06:54,  9.97it/s]   3%|▎         | 129/4192 [00:09<03:35, 18.89it/s]  5%|▍         | 193/4192 [00:11<02:30, 26.64it/s]  6%|▌         | 257/4192 [00:12<01:59, 33.00it/s]  8%|▊         | 321/4192 [00:13<01:41, 38.17it/s]  9%|▉         | 385/4192 [00:14<01:30, 42.13it/s] 11%|█         | 449/4192 [00:16<01:22, 45.11it/s] 12%|█▏        | 513/4192 [00:17<01:17, 47.29it/s] 14%|█▍        | 577/4192 [00:18<01:13, 49.01it/s] 15%|█▌        | 641/4192 [00:19<01:10, 50.12it/s] 17%|█▋        | 705/4192 [00:20<01:08, 51.10it/s] 18%|█▊        | 769/4192 [00:22<01:06, 51.79it/s] 20%|█▉        | 833/4192 [00:23<01:04, 52.19it/s] 21%|██▏       | 897/4192 [00:24<01:02, 52.52it/s] 23%|██▎       | 961/4192 [00:25<01:01, 52.77it/s] 24%|██▍       | 1025/4192 [00:26<00:59, 53.16it/s] 26%|██▌       | 1089/4192 [00:28<00:58, 53.39it/s] 28%|██▊       | 1153/4192 [00:29<00:56, 53.57it/s] 29%|██▉       | 1217/4192 [00:30<00:55, 53.67it/s] 31%|███       | 1281/4192 [00:31<00:54, 53.82it/s] 32%|███▏      | 1345/4192 [00:32<00:52, 53.86it/s] 34%|███▎      | 1409/4192 [00:33<00:51, 53.94it/s] 35%|███▌      | 1473/4192 [00:35<00:50, 53.93it/s] 37%|███▋      | 1537/4192 [00:36<00:49, 53.90it/s] 38%|███▊      | 1601/4192 [00:37<00:47, 54.17it/s] 40%|███▉      | 1665/4192 [00:38<00:46, 54.35it/s] 41%|████      | 1729/4192 [00:39<00:45, 54.49it/s] 43%|████▎     | 1793/4192 [00:41<00:44, 54.51it/s] 44%|████▍     | 1857/4192 [00:42<00:42, 54.60it/s] 46%|████▌     | 1921/4192 [00:43<00:41, 54.61it/s] 47%|████▋     | 1985/4192 [00:44<00:40, 54.64it/s] 49%|████▉     | 2049/4192 [00:45<00:39, 54.63it/s] 50%|█████     | 2113/4192 [00:46<00:38, 54.62it/s] 52%|█████▏    | 2177/4192 [00:48<00:36, 54.92it/s] 53%|█████▎    | 2241/4192 [00:49<00:35, 55.08it/s] 55%|█████▍    | 2305/4192 [00:50<00:34, 55.25it/s] 57%|█████▋    | 2369/4192 [00:51<00:32, 55.37it/s] 58%|█████▊    | 2433/4192 [00:52<00:31, 55.44it/s] 60%|█████▉    | 2497/4192 [00:53<00:30, 55.48it/s] 61%|██████    | 2561/4192 [00:54<00:29, 55.50it/s] 63%|██████▎   | 2625/4192 [00:56<00:28, 55.51it/s] 64%|██████▍   | 2689/4192 [00:57<00:27, 55.55it/s] 66%|██████▌   | 2753/4192 [00:58<00:25, 55.83it/s] 67%|██████▋   | 2817/4192 [00:59<00:24, 55.99it/s] 69%|██████▊   | 2881/4192 [01:00<00:23, 56.09it/s] 70%|███████   | 2945/4192 [01:01<00:22, 56.20it/s] 72%|███████▏  | 3009/4192 [01:02<00:21, 56.22it/s] 73%|███████▎  | 3073/4192 [01:04<00:19, 56.26it/s] 75%|███████▍  | 3137/4192 [01:05<00:18, 56.30it/s] 76%|███████▋  | 3201/4192 [01:06<00:17, 56.28it/s] 78%|███████▊  | 3265/4192 [01:07<00:16, 56.50it/s] 79%|███████▉  | 3329/4192 [01:08<00:15, 56.65it/s] 81%|████████  | 3393/4192 [01:09<00:14, 56.72it/s] 82%|████████▏ | 3457/4192 [01:10<00:12, 56.82it/s] 84%|████████▍ | 3521/4192 [01:11<00:11, 56.85it/s] 86%|████████▌ | 3585/4192 [01:13<00:10, 56.89it/s] 87%|████████▋ | 3649/4192 [01:14<00:09, 56.93it/s] 89%|████████▊ | 3713/4192 [01:15<00:08, 56.93it/s] 90%|█████████ | 3777/4192 [01:16<00:07, 57.22it/s] 92%|█████████▏| 3841/4192 [01:17<00:06, 57.36it/s] 93%|█████████▎| 3905/4192 [01:18<00:04, 57.48it/s] 95%|█████████▍| 3969/4192 [01:19<00:03, 57.56it/s] 96%|█████████▌| 4033/4192 [01:20<00:02, 57.61it/s] 98%|█████████▊| 4097/4192 [01:21<00:01, 57.98it/s] 99%|█████████▉| 4161/4192 [01:22<00:00, 63.62it/s]100%|██████████| 4192/4192 [01:22<00:00, 50.67it/s]
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 64
hf (pretrained=mistralai/Mixtral-8x7B-v0.1,parallelize=True,load_in_4bit=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)
|Tasks|Version|Filter|n-shot| Metric |Value |   |Stderr|
|-----|-------|------|-----:|--------|-----:|---|-----:|
|b4bqa|Yaml   |none  |     0|acc     |0.9714|±  |0.0052|
|     |       |none  |     0|acc_norm|0.9714|±  |0.0052|

Running evaluation for model meta-llama/Llama-2-70B-hf
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-06-13:22:09:59,415 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-06-13:22:09:59,415 INFO     [utils.py:161] NumExpr defaulting to 8 threads.
2024-06-13:22:09:59,650 INFO     [config.py:58] PyTorch version 2.3.0 available.
2024-06-13:22:10:09,293 INFO     [__main__.py:156] Verbosity set to INFO
2024-06-13:22:10:13,963 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-06-13:22:10:41,673 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-06-13:22:10:41,675 INFO     [__main__.py:229] Selected Tasks: ['b4bqa']
2024-06-13:22:10:41,734 WARNING  [other.py:349] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Downloading shards:   0%|          | 0/15 [00:00<?, ?it/s]Downloading shards:   7%|▋         | 1/15 [03:59<55:57, 239.80s/it]Downloading shards:  13%|█▎        | 2/15 [07:43<49:55, 230.45s/it]Downloading shards:  20%|██        | 3/15 [11:37<46:25, 232.15s/it]Downloading shards:  27%|██▋       | 4/15 [15:26<42:19, 230.89s/it]Downloading shards:  33%|███▎      | 5/15 [19:02<37:32, 225.30s/it]Downloading shards:  40%|████      | 6/15 [22:42<33:32, 223.57s/it]Downloading shards:  47%|████▋     | 7/15 [26:20<29:33, 221.67s/it]Downloading shards:  53%|█████▎    | 8/15 [29:48<25:22, 217.43s/it]Downloading shards:  60%|██████    | 9/15 [33:16<21:27, 214.60s/it]Downloading shards:  67%|██████▋   | 10/15 [36:41<17:37, 211.56s/it]Downloading shards:  73%|███████▎  | 11/15 [40:14<14:08, 212.05s/it]Downloading shards:  80%|████████  | 12/15 [43:35<10:26, 208.68s/it]Downloading shards:  87%|████████▋ | 13/15 [46:56<06:52, 206.22s/it]Downloading shards:  93%|█████████▎| 14/15 [50:02<03:20, 200.22s/it]Downloading shards: 100%|██████████| 15/15 [50:17<00:00, 144.19s/it]Downloading shards: 100%|██████████| 15/15 [50:17<00:00, 201.14s/it]
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [02:42<37:48, 162.04s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [03:08<17:53, 82.58s/it] Loading checkpoint shards:  20%|██        | 3/15 [03:30<10:57, 54.80s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [03:56<07:55, 43.23s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [04:17<05:52, 35.23s/it]Loading checkpoint shards:  40%|████      | 6/15 [04:45<04:55, 32.83s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [07:35<10:22, 77.80s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [07:58<07:02, 60.36s/it]Loading checkpoint shards:  60%|██████    | 9/15 [08:24<04:56, 49.35s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [08:45<03:23, 40.71s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [09:13<02:27, 36.93s/it]Loading checkpoint shards:  80%|████████  | 12/15 [09:39<01:40, 33.42s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [10:01<01:00, 30.13s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [10:23<00:27, 27.75s/it]Loading checkpoint shards: 100%|██████████| 15/15 [10:25<00:00, 19.81s/it]Loading checkpoint shards: 100%|██████████| 15/15 [10:25<00:00, 41.69s/it]
Downloading readme:   0%|          | 0.00/31.0 [00:00<?, ?B/s]Downloading readme: 100%|██████████| 31.0/31.0 [00:00<00:00, 133kB/s]
Downloading data:   0%|          | 0.00/161k [00:00<?, ?B/s]Downloading data: 100%|██████████| 161k/161k [00:00<00:00, 1.12MB/s]Downloading data: 100%|██████████| 161k/161k [00:00<00:00, 1.12MB/s]
Generating test split:   0%|          | 0/1048 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 1048/1048 [00:00<00:00, 3152.27 examples/s]Generating test split: 100%|██████████| 1048/1048 [00:00<00:00, 3083.37 examples/s]
2024-06-13:23:11:31,341 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-13:23:11:31,341 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-13:23:11:31,378 INFO     [task.py:343] Building contexts for task on rank 0...
2024-06-13:23:11:31,443 INFO     [evaluator.py:314] Running loglikelihood requests
  0%|          | 0/4192 [00:00<?, ?it/s]  0%|          | 1/4192 [00:31<36:44:47, 31.56s/it]  2%|▏         | 65/4192 [00:36<28:25,  2.42it/s]    3%|▎         | 129/4192 [00:40<14:28,  4.68it/s]  5%|▍         | 193/4192 [00:45<09:54,  6.73it/s]  6%|▌         | 257/4192 [00:49<07:44,  8.47it/s]  8%|▊         | 321/4192 [00:54<06:31,  9.90it/s]  9%|▉         | 385/4192 [00:58<05:45, 11.02it/s] 11%|█         | 449/4192 [01:03<05:15, 11.88it/s] 12%|█▏        | 513/4192 [01:08<04:53, 12.52it/s] 14%|█▍        | 577/4192 [01:12<04:38, 12.98it/s] 15%|█▌        | 641/4192 [01:16<04:24, 13.43it/s] 17%|█▋        | 705/4192 [01:21<04:13, 13.76it/s] 18%|█▊        | 769/4192 [01:25<04:04, 13.99it/s] 20%|█▉        | 833/4192 [01:30<03:57, 14.15it/s] 21%|██▏       | 897/4192 [01:34<03:50, 14.27it/s] 23%|██▎       | 961/4192 [01:38<03:45, 14.35it/s] 24%|██▍       | 1025/4192 [01:43<03:39, 14.42it/s] 26%|██▌       | 1089/4192 [01:47<03:34, 14.47it/s] 28%|██▊       | 1153/4192 [01:52<03:29, 14.49it/s] 29%|██▉       | 1217/4192 [01:56<03:24, 14.52it/s] 31%|███       | 1281/4192 [02:00<03:20, 14.53it/s] 32%|███▏      | 1345/4192 [02:05<03:15, 14.54it/s] 34%|███▎      | 1409/4192 [02:09<03:11, 14.54it/s] 35%|███▌      | 1473/4192 [02:14<03:06, 14.54it/s] 37%|███▋      | 1537/4192 [02:18<03:00, 14.67it/s] 38%|███▊      | 1601/4192 [02:22<02:55, 14.76it/s] 40%|███▉      | 1665/4192 [02:26<02:50, 14.83it/s] 41%|████      | 1729/4192 [02:31<02:45, 14.87it/s] 43%|████▎     | 1793/4192 [02:35<02:40, 14.90it/s] 44%|████▍     | 1857/4192 [02:39<02:36, 14.92it/s] 46%|████▌     | 1921/4192 [02:44<02:32, 14.93it/s] 47%|████▋     | 1985/4192 [02:48<02:27, 14.94it/s] 49%|████▉     | 2049/4192 [02:52<02:23, 14.95it/s] 50%|█████     | 2113/4192 [02:56<02:18, 14.97it/s] 52%|█████▏    | 2177/4192 [03:01<02:14, 14.98it/s] 53%|█████▎    | 2241/4192 [03:05<02:10, 14.99it/s] 55%|█████▍    | 2305/4192 [03:09<02:05, 14.99it/s] 57%|█████▋    | 2369/4192 [03:13<02:01, 14.99it/s] 58%|█████▊    | 2433/4192 [03:18<01:57, 14.99it/s] 60%|█████▉    | 2497/4192 [03:22<01:53, 14.99it/s] 61%|██████    | 2561/4192 [03:26<01:48, 15.00it/s] 63%|██████▎   | 2625/4192 [03:30<01:44, 15.00it/s] 64%|██████▍   | 2689/4192 [03:35<01:39, 15.16it/s] 66%|██████▌   | 2753/4192 [03:39<01:34, 15.27it/s] 67%|██████▋   | 2817/4192 [03:43<01:29, 15.35it/s] 69%|██████▊   | 2881/4192 [03:47<01:25, 15.41it/s] 70%|███████   | 2945/4192 [03:51<01:20, 15.45it/s] 72%|███████▏  | 3009/4192 [03:55<01:16, 15.48it/s] 73%|███████▎  | 3073/4192 [03:59<01:12, 15.51it/s] 75%|███████▍  | 3137/4192 [04:03<01:07, 15.52it/s] 76%|███████▋  | 3201/4192 [04:08<01:03, 15.52it/s] 78%|███████▊  | 3265/4192 [04:12<00:59, 15.52it/s] 79%|███████▉  | 3329/4192 [04:16<00:55, 15.54it/s] 81%|████████  | 3393/4192 [04:20<00:51, 15.55it/s] 82%|████████▏ | 3457/4192 [04:24<00:47, 15.56it/s] 84%|████████▍ | 3521/4192 [04:28<00:43, 15.55it/s] 86%|████████▌ | 3585/4192 [04:32<00:39, 15.56it/s] 87%|████████▋ | 3649/4192 [04:36<00:34, 15.56it/s] 89%|████████▊ | 3713/4192 [04:40<00:30, 15.85it/s] 90%|█████████ | 3777/4192 [04:44<00:25, 16.07it/s] 92%|█████████▏| 3841/4192 [04:48<00:21, 16.22it/s] 93%|█████████▎| 3905/4192 [04:52<00:17, 16.32it/s] 95%|█████████▍| 3969/4192 [04:56<00:13, 16.39it/s] 96%|█████████▌| 4033/4192 [04:59<00:09, 16.47it/s] 98%|█████████▊| 4097/4192 [05:03<00:05, 16.52it/s] 99%|█████████▉| 4161/4192 [05:05<00:01, 19.23it/s]100%|██████████| 4192/4192 [05:05<00:00, 13.70it/s]
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 64
hf (pretrained=meta-llama/Llama-2-70B-hf,parallelize=True,load_in_4bit=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)
|Tasks|Version|Filter|n-shot| Metric |Value |   |Stderr|
|-----|-------|------|-----:|--------|-----:|---|-----:|
|b4bqa|Yaml   |none  |     0|acc     |0.9361|±  |0.0076|
|     |       |none  |     0|acc_norm|0.9361|±  |0.0076|

Running evaluation for model meta-llama/Meta-Llama-3-70B
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-06-13:23:16:49,642 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-06-13:23:16:49,642 INFO     [utils.py:161] NumExpr defaulting to 8 threads.
2024-06-13:23:16:50,417 INFO     [config.py:58] PyTorch version 2.3.0 available.
2024-06-13:23:17:06,264 INFO     [__main__.py:156] Verbosity set to INFO
2024-06-13:23:17:13,133 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-06-13:23:17:38,282 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-06-13:23:17:38,284 INFO     [__main__.py:229] Selected Tasks: ['b4bqa']
2024-06-13:23:17:38,345 WARNING  [other.py:349] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Downloading shards:   0%|          | 0/30 [00:00<?, ?it/s]Downloading shards:   3%|▎         | 1/30 [00:19<09:34, 19.80s/it]Downloading shards:   7%|▋         | 2/30 [00:44<10:37, 22.75s/it]Downloading shards:  10%|█         | 3/30 [01:12<11:18, 25.14s/it]Downloading shards:  13%|█▎        | 4/30 [01:39<11:09, 25.74s/it]Downloading shards:  17%|█▋        | 5/30 [02:00<10:02, 24.12s/it]Downloading shards:  20%|██        | 6/30 [02:29<10:17, 25.74s/it]Downloading shards:  23%|██▎       | 7/30 [02:53<09:43, 25.35s/it]Downloading shards:  27%|██▋       | 8/30 [03:18<09:10, 25.01s/it]Downloading shards:  30%|███       | 9/30 [03:41<08:35, 24.53s/it]Downloading shards:  33%|███▎      | 10/30 [04:08<08:24, 25.21s/it]Downloading shards:  37%|███▋      | 11/30 [04:37<08:20, 26.33s/it]Downloading shards:  40%|████      | 12/30 [05:02<07:50, 26.11s/it]Downloading shards:  43%|████▎     | 13/30 [05:32<07:40, 27.09s/it]Downloading shards:  47%|████▋     | 14/30 [05:57<07:04, 26.54s/it]Downloading shards:  50%|█████     | 15/30 [06:23<06:34, 26.32s/it]Downloading shards:  53%|█████▎    | 16/30 [06:54<06:28, 27.76s/it]Downloading shards:  57%|█████▋    | 17/30 [07:22<06:00, 27.74s/it]Downloading shards:  60%|██████    | 18/30 [07:50<05:34, 27.89s/it]Downloading shards:  63%|██████▎   | 19/30 [08:17<05:04, 27.65s/it]Downloading shards:  67%|██████▋   | 20/30 [08:42<04:29, 26.91s/it]Downloading shards:  70%|███████   | 21/30 [09:05<03:50, 25.60s/it]Downloading shards:  73%|███████▎  | 22/30 [09:35<03:35, 26.96s/it]Downloading shards:  77%|███████▋  | 23/30 [10:02<03:10, 27.16s/it]Downloading shards:  80%|████████  | 24/30 [10:30<02:44, 27.36s/it]Downloading shards:  83%|████████▎ | 25/30 [11:05<02:27, 29.48s/it]Downloading shards:  87%|████████▋ | 26/30 [11:34<01:58, 29.51s/it]Downloading shards:  90%|█████████ | 27/30 [11:59<01:24, 28.19s/it]Downloading shards:  93%|█████████▎| 28/30 [12:28<00:56, 28.34s/it]Downloading shards:  97%|█████████▋| 29/30 [12:54<00:27, 27.63s/it]Downloading shards: 100%|██████████| 30/30 [13:07<00:00, 23.11s/it]Downloading shards: 100%|██████████| 30/30 [13:07<00:00, 26.24s/it]
Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:14<06:57, 14.38s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:22<05:07, 10.97s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:34<05:08, 11.44s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:43<04:29, 10.36s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:52<04:03,  9.73s/it]Loading checkpoint shards:  20%|██        | 6/30 [01:05<04:20, 10.84s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [01:19<04:37, 12.07s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [01:28<04:02, 11.04s/it]Loading checkpoint shards:  30%|███       | 9/30 [01:37<03:36, 10.29s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [01:45<03:13,  9.67s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [01:53<02:54,  9.20s/it]Loading checkpoint shards:  40%|████      | 12/30 [02:02<02:41,  8.98s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [02:14<02:47,  9.84s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [02:23<02:35,  9.73s/it]Loading checkpoint shards:  50%|█████     | 15/30 [02:35<02:33, 10.25s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [02:43<02:15,  9.67s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [02:52<02:02,  9.42s/it]Loading checkpoint shards:  60%|██████    | 18/30 [03:00<01:50,  9.21s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [03:09<01:38,  8.95s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [03:17<01:26,  8.67s/it]Loading checkpoint shards:  70%|███████   | 21/30 [03:25<01:17,  8.61s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [03:33<01:07,  8.50s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [03:42<01:00,  8.58s/it]Loading checkpoint shards:  80%|████████  | 24/30 [03:51<00:51,  8.61s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [03:59<00:42,  8.44s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [04:08<00:34,  8.57s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [04:17<00:26,  8.87s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [04:26<00:17,  8.87s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [04:36<00:09,  9.22s/it]Loading checkpoint shards: 100%|██████████| 30/30 [04:40<00:00,  7.59s/it]Loading checkpoint shards: 100%|██████████| 30/30 [04:40<00:00,  9.35s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-13:23:35:31,059 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-13:23:35:31,060 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-13:23:35:31,094 INFO     [task.py:343] Building contexts for task on rank 0...
2024-06-13:23:35:31,159 INFO     [evaluator.py:314] Running loglikelihood requests
  0%|          | 0/4192 [00:00<?, ?it/s]  0%|          | 1/4192 [00:27<32:33:23, 27.97s/it]  2%|▏         | 65/4192 [00:32<25:32,  2.69it/s]    3%|▎         | 129/4192 [00:36<13:10,  5.14it/s]  5%|▍         | 193/4192 [00:41<09:09,  7.28it/s]  6%|▌         | 257/4192 [00:45<07:11,  9.11it/s]  8%|▊         | 321/4192 [00:49<06:05, 10.59it/s]  9%|▉         | 385/4192 [00:54<05:24, 11.74it/s] 11%|█         | 449/4192 [00:58<04:56, 12.61it/s] 12%|█▏        | 513/4192 [01:02<04:37, 13.26it/s] 14%|█▍        | 577/4192 [01:07<04:23, 13.73it/s] 15%|█▌        | 641/4192 [01:11<04:12, 14.07it/s] 17%|█▋        | 705/4192 [01:15<04:03, 14.31it/s] 18%|█▊        | 769/4192 [01:19<03:56, 14.48it/s] 20%|█▉        | 833/4192 [01:24<03:50, 14.59it/s] 21%|██▏       | 897/4192 [01:28<03:42, 14.81it/s] 23%|██▎       | 961/4192 [01:32<03:36, 14.96it/s] 24%|██▍       | 1025/4192 [01:36<03:30, 15.07it/s] 26%|██▌       | 1089/4192 [01:40<03:24, 15.15it/s] 28%|██▊       | 1153/4192 [01:45<03:19, 15.21it/s] 29%|██▉       | 1217/4192 [01:49<03:15, 15.24it/s] 31%|███       | 1281/4192 [01:53<03:10, 15.26it/s] 32%|███▏      | 1345/4192 [01:57<03:06, 15.27it/s] 34%|███▎      | 1409/4192 [02:01<03:02, 15.28it/s] 35%|███▌      | 1473/4192 [02:06<02:57, 15.31it/s] 37%|███▋      | 1537/4192 [02:10<02:53, 15.32it/s] 38%|███▊      | 1601/4192 [02:14<02:48, 15.33it/s] 40%|███▉      | 1665/4192 [02:18<02:44, 15.34it/s] 41%|████      | 1729/4192 [02:22<02:40, 15.34it/s] 43%|████▎     | 1793/4192 [02:26<02:36, 15.35it/s] 44%|████▍     | 1857/4192 [02:31<02:32, 15.35it/s] 46%|████▌     | 1921/4192 [02:35<02:27, 15.36it/s] 47%|████▋     | 1985/4192 [02:39<02:23, 15.37it/s] 49%|████▉     | 2049/4192 [02:43<02:16, 15.66it/s] 50%|█████     | 2113/4192 [02:47<02:10, 15.88it/s] 52%|█████▏    | 2177/4192 [02:51<02:05, 16.04it/s] 53%|█████▎    | 2241/4192 [02:54<02:00, 16.16it/s] 55%|█████▍    | 2305/4192 [02:58<01:56, 16.24it/s] 57%|█████▋    | 2369/4192 [03:02<01:51, 16.30it/s] 58%|█████▊    | 2433/4192 [03:06<01:47, 16.34it/s] 60%|█████▉    | 2497/4192 [03:10<01:43, 16.37it/s] 61%|██████    | 2561/4192 [03:14<01:39, 16.38it/s] 63%|██████▎   | 2625/4192 [03:18<01:35, 16.39it/s] 64%|██████▍   | 2689/4192 [03:22<01:31, 16.41it/s] 66%|██████▌   | 2753/4192 [03:26<01:27, 16.45it/s] 67%|██████▋   | 2817/4192 [03:29<01:23, 16.47it/s] 69%|██████▊   | 2881/4192 [03:33<01:19, 16.48it/s] 70%|███████   | 2945/4192 [03:37<01:15, 16.49it/s] 72%|███████▏  | 3009/4192 [03:41<01:11, 16.49it/s] 73%|███████▎  | 3073/4192 [03:45<01:07, 16.49it/s] 75%|███████▍  | 3137/4192 [03:49<01:03, 16.49it/s] 76%|███████▋  | 3201/4192 [03:53<01:00, 16.48it/s] 78%|███████▊  | 3265/4192 [03:57<00:56, 16.47it/s] 79%|███████▉  | 3329/4192 [04:01<00:52, 16.46it/s] 81%|████████  | 3393/4192 [04:04<00:48, 16.45it/s] 82%|████████▏ | 3457/4192 [04:08<00:44, 16.59it/s] 84%|████████▍ | 3521/4192 [04:12<00:40, 16.70it/s] 86%|████████▌ | 3585/4192 [04:16<00:36, 16.78it/s] 87%|████████▋ | 3649/4192 [04:20<00:32, 16.83it/s] 89%|████████▊ | 3713/4192 [04:23<00:28, 16.86it/s] 90%|█████████ | 3777/4192 [04:27<00:24, 16.89it/s] 92%|█████████▏| 3841/4192 [04:31<00:20, 16.91it/s] 93%|█████████▎| 3905/4192 [04:35<00:16, 16.96it/s] 95%|█████████▍| 3969/4192 [04:38<00:13, 16.99it/s] 96%|█████████▌| 4033/4192 [04:42<00:09, 17.02it/s] 98%|█████████▊| 4097/4192 [04:46<00:05, 17.04it/s] 99%|█████████▉| 4161/4192 [04:48<00:01, 19.68it/s]100%|██████████| 4192/4192 [04:48<00:00, 14.53it/s]
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 64
hf (pretrained=meta-llama/Meta-Llama-3-70B,parallelize=True,load_in_4bit=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)
|Tasks|Version|Filter|n-shot| Metric |Value |   |Stderr|
|-----|-------|------|-----:|--------|-----:|---|-----:|
|b4bqa|Yaml   |none  |     0|acc     |0.9857|±  |0.0037|
|     |       |none  |     0|acc_norm|0.9857|±  |0.0037|

Running evaluation for model Qwen/Qwen2-72B
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-06-13:23:40:27,969 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-06-13:23:40:27,969 INFO     [utils.py:161] NumExpr defaulting to 8 threads.
2024-06-13:23:40:28,220 INFO     [config.py:58] PyTorch version 2.3.0 available.
2024-06-13:23:40:38,011 INFO     [__main__.py:156] Verbosity set to INFO
2024-06-13:23:40:42,854 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-06-13:23:41:10,079 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-06-13:23:41:10,082 INFO     [__main__.py:229] Selected Tasks: ['b4bqa']
2024-06-13:23:41:10,143 WARNING  [other.py:349] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Downloading shards:   0%|          | 0/37 [00:00<?, ?it/s]Downloading shards:   3%|▎         | 1/37 [00:29<17:32, 29.23s/it]Downloading shards:   5%|▌         | 2/37 [00:55<15:58, 27.38s/it]Downloading shards:   8%|▊         | 3/37 [01:21<15:06, 26.66s/it]Downloading shards:  11%|█         | 4/37 [01:46<14:27, 26.30s/it]Downloading shards:  14%|█▎        | 5/37 [02:12<13:54, 26.07s/it]Downloading shards:  16%|█▌        | 6/37 [02:39<13:37, 26.38s/it]Downloading shards:  19%|█▉        | 7/37 [03:06<13:18, 26.62s/it]
