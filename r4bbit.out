usage: huggingface-cli <command> [<args>] login [-h] [--token TOKEN]
                                                [--add-to-git-credential]
huggingface-cli <command> [<args>] login: error: argument --token: expected one argument
Running evaluation for model microsoft/phi-1
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-06-14:07:13:14,780 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-06-14:07:13:14,780 INFO     [utils.py:161] NumExpr defaulting to 8 threads.
2024-06-14:07:13:16,060 INFO     [config.py:58] PyTorch version 2.3.0 available.
2024-06-14:07:13:35,179 INFO     [__main__.py:156] Verbosity set to INFO
2024-06-14:07:13:44,160 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-06-14:07:14:11,504 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-06-14:07:14:11,507 INFO     [__main__.py:229] Selected Tasks: ['b4bqa']
2024-06-14:07:14:11,577 WARNING  [other.py:349] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2024-06-14:07:15:16,288 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-14:07:15:16,288 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-14:07:15:16,324 INFO     [task.py:343] Building contexts for task on rank 0...
2024-06-14:07:15:16,393 INFO     [evaluator.py:314] Running loglikelihood requests
  0%|          | 0/4192 [00:00<?, ?it/s]  0%|          | 1/4192 [00:02<3:21:59,  2.89s/it]  2%|▏         | 65/4192 [00:03<02:19, 29.61it/s]   3%|▎         | 129/4192 [00:03<01:02, 64.87it/s]  5%|▍         | 193/4192 [00:03<00:37, 105.28it/s]  6%|▌         | 257/4192 [00:03<00:26, 149.33it/s]  8%|▊         | 321/4192 [00:03<00:19, 194.31it/s]  9%|▉         | 385/4192 [00:03<00:16, 237.55it/s] 11%|█         | 449/4192 [00:03<00:13, 276.49it/s] 12%|█▏        | 513/4192 [00:04<00:11, 310.67it/s] 14%|█▍        | 577/4192 [00:04<00:10, 338.76it/s] 15%|█▌        | 641/4192 [00:04<00:09, 360.76it/s] 17%|█▋        | 705/4192 [00:04<00:09, 377.64it/s] 18%|█▊        | 769/4192 [00:04<00:08, 390.25it/s] 20%|█▉        | 833/4192 [00:04<00:08, 399.58it/s] 21%|██▏       | 897/4192 [00:05<00:08, 406.30it/s] 23%|██▎       | 961/4192 [00:05<00:07, 410.30it/s] 24%|██▍       | 1025/4192 [00:05<00:07, 419.20it/s] 26%|██▌       | 1089/4192 [00:05<00:07, 425.60it/s] 28%|██▊       | 1153/4192 [00:05<00:07, 430.04it/s] 29%|██▉       | 1217/4192 [00:05<00:06, 433.25it/s] 31%|███       | 1281/4192 [00:05<00:06, 435.65it/s] 32%|███▏      | 1345/4192 [00:06<00:06, 437.37it/s] 34%|███▎      | 1409/4192 [00:06<00:06, 438.68it/s] 35%|███▌      | 1473/4192 [00:06<00:06, 438.69it/s] 37%|███▋      | 1537/4192 [00:06<00:06, 440.25it/s] 38%|███▊      | 1601/4192 [00:06<00:05, 441.49it/s] 40%|███▉      | 1665/4192 [00:06<00:05, 442.43it/s] 41%|████      | 1729/4192 [00:06<00:05, 442.72it/s] 43%|████▎     | 1793/4192 [00:07<00:05, 442.96it/s] 44%|████▍     | 1857/4192 [00:07<00:05, 442.85it/s] 46%|████▌     | 1921/4192 [00:07<00:05, 442.97it/s] 47%|████▋     | 1985/4192 [00:07<00:04, 443.10it/s] 49%|████▉     | 2049/4192 [00:07<00:04, 442.44it/s] 50%|█████     | 2113/4192 [00:07<00:04, 445.33it/s] 52%|█████▏    | 2177/4192 [00:07<00:04, 447.54it/s] 53%|█████▎    | 2241/4192 [00:08<00:04, 449.13it/s] 55%|█████▍    | 2305/4192 [00:08<00:04, 450.13it/s] 57%|█████▋    | 2369/4192 [00:08<00:04, 451.09it/s] 58%|█████▊    | 2433/4192 [00:08<00:03, 451.52it/s] 60%|█████▉    | 2497/4192 [00:08<00:03, 451.18it/s] 61%|██████    | 2561/4192 [00:08<00:03, 450.98it/s] 63%|██████▎   | 2625/4192 [00:08<00:03, 450.96it/s] 64%|██████▍   | 2689/4192 [00:09<00:03, 450.82it/s] 66%|██████▌   | 2753/4192 [00:09<00:03, 449.81it/s] 67%|██████▋   | 2817/4192 [00:09<00:03, 448.55it/s] 69%|██████▊   | 2881/4192 [00:09<00:02, 447.64it/s] 70%|███████   | 2945/4192 [00:09<00:02, 447.15it/s] 72%|███████▏  | 3009/4192 [00:09<00:02, 447.21it/s] 73%|███████▎  | 3073/4192 [00:09<00:02, 446.89it/s] 75%|███████▍  | 3137/4192 [00:10<00:02, 446.60it/s] 76%|███████▋  | 3201/4192 [00:10<00:02, 446.44it/s] 78%|███████▊  | 3265/4192 [00:10<00:02, 445.90it/s] 79%|███████▉  | 3329/4192 [00:10<00:01, 460.31it/s] 81%|████████  | 3393/4192 [00:10<00:01, 470.79it/s] 82%|████████▏ | 3457/4192 [00:10<00:01, 478.36it/s] 84%|████████▍ | 3521/4192 [00:10<00:01, 483.92it/s] 86%|████████▌ | 3585/4192 [00:10<00:01, 487.97it/s] 87%|████████▋ | 3649/4192 [00:11<00:01, 490.63it/s] 89%|████████▊ | 3713/4192 [00:11<00:00, 492.09it/s] 90%|█████████ | 3777/4192 [00:11<00:00, 495.29it/s] 92%|█████████▏| 3841/4192 [00:11<00:00, 497.36it/s] 93%|█████████▎| 3905/4192 [00:11<00:00, 498.86it/s] 95%|█████████▍| 3969/4192 [00:11<00:00, 500.13it/s] 96%|█████████▌| 4033/4192 [00:11<00:00, 499.88it/s] 98%|█████████▊| 4097/4192 [00:12<00:00, 503.14it/s]100%|██████████| 4192/4192 [00:12<00:00, 346.50it/s]
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 64
hf (pretrained=microsoft/phi-1,parallelize=True,load_in_4bit=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)
|Tasks|Version|Filter|n-shot| Metric |Value |   |Stderr|
|-----|-------|------|-----:|--------|-----:|---|-----:|
|b4bqa|Yaml   |none  |     0|acc     |0.2576|±  |0.0135|
|     |       |none  |     0|acc_norm|0.2576|±  |0.0135|

Running evaluation for model microsoft/phi-1_5
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-06-14:07:15:35,995 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-06-14:07:15:35,995 INFO     [utils.py:161] NumExpr defaulting to 8 threads.
2024-06-14:07:15:36,229 INFO     [config.py:58] PyTorch version 2.3.0 available.
2024-06-14:07:15:43,988 INFO     [__main__.py:156] Verbosity set to INFO
2024-06-14:07:15:48,687 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-06-14:07:16:13,983 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-06-14:07:16:13,986 INFO     [__main__.py:229] Selected Tasks: ['b4bqa']
2024-06-14:07:16:14,038 WARNING  [other.py:349] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2024-06-14:07:16:36,147 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-14:07:16:36,147 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-14:07:16:36,180 INFO     [task.py:343] Building contexts for task on rank 0...
2024-06-14:07:16:36,245 INFO     [evaluator.py:314] Running loglikelihood requests
  0%|          | 0/4192 [00:00<?, ?it/s]  0%|          | 1/4192 [00:01<1:27:48,  1.26s/it]  2%|▏         | 65/4192 [00:01<01:06, 62.38it/s]   3%|▎         | 129/4192 [00:01<00:32, 124.03it/s]  5%|▍         | 193/4192 [00:01<00:21, 181.89it/s]  6%|▌         | 257/4192 [00:01<00:16, 233.94it/s]  8%|▊         | 321/4192 [00:02<00:13, 278.04it/s]  9%|▉         | 385/4192 [00:02<00:12, 313.82it/s] 11%|█         | 449/4192 [00:02<00:10, 341.43it/s] 12%|█▏        | 513/4192 [00:02<00:10, 363.73it/s] 14%|█▍        | 577/4192 [00:02<00:09, 380.55it/s] 15%|█▌        | 641/4192 [00:02<00:09, 392.50it/s] 17%|█▋        | 705/4192 [00:02<00:08, 401.20it/s] 18%|█▊        | 769/4192 [00:03<00:08, 407.53it/s] 20%|█▉        | 833/4192 [00:03<00:08, 412.38it/s] 21%|██▏       | 897/4192 [00:03<00:07, 416.09it/s] 23%|██▎       | 961/4192 [00:03<00:07, 417.64it/s] 24%|██▍       | 1025/4192 [00:03<00:07, 424.83it/s] 26%|██▌       | 1089/4192 [00:03<00:07, 429.49it/s] 28%|██▊       | 1153/4192 [00:03<00:07, 432.87it/s] 29%|██▉       | 1217/4192 [00:04<00:06, 435.01it/s] 31%|███       | 1281/4192 [00:04<00:06, 436.79it/s] 32%|███▏      | 1345/4192 [00:04<00:06, 438.08it/s] 34%|███▎      | 1409/4192 [00:04<00:06, 436.94it/s] 35%|███▌      | 1473/4192 [00:04<00:06, 437.23it/s] 37%|███▋      | 1537/4192 [00:04<00:06, 439.00it/s] 38%|███▊      | 1601/4192 [00:05<00:05, 440.49it/s] 40%|███▉      | 1665/4192 [00:05<00:05, 441.73it/s] 41%|████      | 1729/4192 [00:05<00:05, 442.44it/s] 43%|████▎     | 1793/4192 [00:05<00:05, 442.84it/s] 44%|████▍     | 1857/4192 [00:05<00:05, 443.08it/s] 46%|████▌     | 1921/4192 [00:05<00:05, 443.28it/s] 47%|████▋     | 1985/4192 [00:05<00:04, 443.31it/s] 49%|████▉     | 2049/4192 [00:06<00:04, 442.37it/s] 50%|█████     | 2113/4192 [00:06<00:04, 445.14it/s] 52%|█████▏    | 2177/4192 [00:06<00:04, 447.03it/s] 53%|█████▎    | 2241/4192 [00:06<00:04, 448.13it/s] 55%|█████▍    | 2305/4192 [00:06<00:04, 448.92it/s] 57%|█████▋    | 2369/4192 [00:06<00:04, 449.37it/s] 58%|█████▊    | 2433/4192 [00:06<00:03, 449.71it/s] 60%|█████▉    | 2497/4192 [00:07<00:03, 449.89it/s] 61%|██████    | 2561/4192 [00:07<00:03, 450.07it/s] 63%|██████▎   | 2625/4192 [00:07<00:03, 450.21it/s] 64%|██████▍   | 2689/4192 [00:07<00:03, 450.27it/s] 66%|██████▌   | 2753/4192 [00:07<00:03, 449.38it/s] 67%|██████▋   | 2817/4192 [00:07<00:03, 448.18it/s] 69%|██████▊   | 2881/4192 [00:07<00:02, 447.19it/s] 70%|███████   | 2945/4192 [00:08<00:02, 446.54it/s] 72%|███████▏  | 3009/4192 [00:08<00:02, 446.10it/s] 73%|███████▎  | 3073/4192 [00:08<00:02, 445.82it/s] 75%|███████▍  | 3137/4192 [00:08<00:02, 445.91it/s] 76%|███████▋  | 3201/4192 [00:08<00:02, 445.81it/s] 78%|███████▊  | 3265/4192 [00:08<00:02, 445.36it/s] 79%|███████▉  | 3329/4192 [00:08<00:01, 459.54it/s] 81%|████████  | 3393/4192 [00:08<00:01, 469.86it/s] 82%|████████▏ | 3457/4192 [00:09<00:01, 477.47it/s] 84%|████████▍ | 3521/4192 [00:09<00:01, 483.33it/s] 86%|████████▌ | 3585/4192 [00:09<00:01, 487.40it/s] 87%|████████▋ | 3649/4192 [00:09<00:01, 490.25it/s] 89%|████████▊ | 3713/4192 [00:09<00:00, 491.69it/s] 90%|█████████ | 3777/4192 [00:09<00:00, 494.83it/s] 92%|█████████▏| 3841/4192 [00:09<00:00, 497.02it/s] 93%|█████████▎| 3905/4192 [00:10<00:00, 498.57it/s] 95%|█████████▍| 3969/4192 [00:10<00:00, 499.54it/s] 96%|█████████▌| 4033/4192 [00:10<00:00, 498.80it/s] 98%|█████████▊| 4097/4192 [00:10<00:00, 501.84it/s]100%|██████████| 4192/4192 [00:10<00:00, 400.42it/s]
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 64
hf (pretrained=microsoft/phi-1_5,parallelize=True,load_in_4bit=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)
|Tasks|Version|Filter|n-shot| Metric |Value |   |Stderr|
|-----|-------|------|-----:|--------|-----:|---|-----:|
|b4bqa|Yaml   |none  |     0|acc     |0.3788|±  | 0.015|
|     |       |none  |     0|acc_norm|0.3788|±  | 0.015|

Running evaluation for model microsoft/phi-2
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-06-14:07:16:52,965 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-06-14:07:16:52,965 INFO     [utils.py:161] NumExpr defaulting to 8 threads.
2024-06-14:07:16:53,197 INFO     [config.py:58] PyTorch version 2.3.0 available.
2024-06-14:07:17:00,865 INFO     [__main__.py:156] Verbosity set to INFO
2024-06-14:07:17:05,534 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-06-14:07:17:32,090 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-06-14:07:17:32,092 INFO     [__main__.py:229] Selected Tasks: ['b4bqa']
2024-06-14:07:17:32,142 WARNING  [other.py:349] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:  50%|█████     | 1/2 [00:25<00:25, 25.16s/it]Downloading shards: 100%|██████████| 2/2 [00:29<00:00, 12.99s/it]Downloading shards: 100%|██████████| 2/2 [00:29<00:00, 14.82s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  5.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.70s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-14:07:18:18,216 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-14:07:18:18,216 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-14:07:18:18,251 INFO     [task.py:343] Building contexts for task on rank 0...
2024-06-14:07:18:18,315 INFO     [evaluator.py:314] Running loglikelihood requests
  0%|          | 0/4192 [00:00<?, ?it/s]  0%|          | 1/4192 [00:02<2:29:01,  2.13s/it]  2%|▏         | 65/4192 [00:02<01:52, 36.64it/s]   3%|▎         | 129/4192 [00:02<00:55, 72.64it/s]  5%|▍         | 193/4192 [00:02<00:37, 106.27it/s]  6%|▌         | 257/4192 [00:03<00:28, 137.72it/s]  8%|▊         | 321/4192 [00:03<00:23, 164.72it/s]  9%|▉         | 385/4192 [00:03<00:20, 186.84it/s] 11%|█         | 449/4192 [00:03<00:18, 204.19it/s] 12%|█▏        | 513/4192 [00:04<00:16, 217.99it/s] 14%|█▍        | 577/4192 [00:04<00:15, 228.44it/s] 15%|█▌        | 641/4192 [00:04<00:15, 236.07it/s] 17%|█▋        | 705/4192 [00:04<00:14, 241.70it/s] 18%|█▊        | 769/4192 [00:05<00:13, 245.68it/s] 20%|█▉        | 833/4192 [00:05<00:13, 248.39it/s] 21%|██▏       | 897/4192 [00:05<00:13, 250.44it/s] 23%|██▎       | 961/4192 [00:05<00:12, 251.53it/s] 24%|██▍       | 1025/4192 [00:06<00:12, 252.38it/s] 26%|██▌       | 1089/4192 [00:06<00:12, 252.93it/s] 28%|██▊       | 1153/4192 [00:06<00:11, 253.45it/s] 29%|██▉       | 1217/4192 [00:06<00:11, 253.64it/s] 31%|███       | 1281/4192 [00:07<00:11, 253.66it/s] 32%|███▏      | 1345/4192 [00:07<00:11, 253.65it/s] 34%|███▎      | 1409/4192 [00:07<00:10, 253.72it/s] 35%|███▌      | 1473/4192 [00:07<00:10, 253.63it/s] 37%|███▋      | 1537/4192 [00:08<00:10, 254.46it/s] 38%|███▊      | 1601/4192 [00:08<00:10, 254.99it/s] 40%|███▉      | 1665/4192 [00:08<00:09, 255.36it/s] 41%|████      | 1729/4192 [00:08<00:09, 255.70it/s] 43%|████▎     | 1793/4192 [00:09<00:09, 255.83it/s] 44%|████▍     | 1857/4192 [00:09<00:09, 256.00it/s] 46%|████▌     | 1921/4192 [00:09<00:08, 256.03it/s] 47%|████▋     | 1985/4192 [00:09<00:08, 256.15it/s] 49%|████▉     | 2049/4192 [00:10<00:08, 255.88it/s] 50%|█████     | 2113/4192 [00:10<00:08, 256.85it/s] 52%|█████▏    | 2177/4192 [00:10<00:07, 257.53it/s] 53%|█████▎    | 2241/4192 [00:10<00:07, 258.06it/s] 55%|█████▍    | 2305/4192 [00:11<00:07, 258.48it/s] 57%|█████▋    | 2369/4192 [00:11<00:07, 258.58it/s] 58%|█████▊    | 2433/4192 [00:11<00:06, 258.73it/s] 60%|█████▉    | 2497/4192 [00:11<00:06, 258.84it/s] 61%|██████    | 2561/4192 [00:12<00:06, 259.10it/s] 63%|██████▎   | 2625/4192 [00:12<00:06, 259.29it/s] 64%|██████▍   | 2689/4192 [00:12<00:05, 259.18it/s] 66%|██████▌   | 2753/4192 [00:12<00:05, 258.73it/s] 67%|██████▋   | 2817/4192 [00:13<00:05, 259.66it/s] 69%|██████▊   | 2881/4192 [00:13<00:05, 260.29it/s] 70%|███████   | 2945/4192 [00:13<00:04, 260.71it/s] 72%|███████▏  | 3009/4192 [00:13<00:04, 260.83it/s] 73%|███████▎  | 3073/4192 [00:14<00:04, 261.11it/s] 75%|███████▍  | 3137/4192 [00:14<00:04, 261.33it/s] 76%|███████▋  | 3201/4192 [00:14<00:03, 261.38it/s] 78%|███████▊  | 3265/4192 [00:14<00:03, 261.16it/s] 79%|███████▉  | 3329/4192 [00:15<00:03, 265.18it/s] 81%|████████  | 3393/4192 [00:15<00:02, 268.00it/s] 82%|████████▏ | 3457/4192 [00:15<00:02, 269.95it/s] 84%|████████▍ | 3521/4192 [00:15<00:02, 271.36it/s] 86%|████████▌ | 3585/4192 [00:16<00:02, 272.43it/s] 87%|████████▋ | 3649/4192 [00:16<00:01, 273.14it/s] 89%|████████▊ | 3713/4192 [00:16<00:01, 273.51it/s] 90%|█████████ | 3777/4192 [00:16<00:01, 274.59it/s] 92%|█████████▏| 3841/4192 [00:16<00:01, 275.41it/s] 93%|█████████▎| 3905/4192 [00:17<00:01, 276.00it/s] 95%|█████████▍| 3969/4192 [00:17<00:00, 276.34it/s] 96%|█████████▌| 4033/4192 [00:17<00:00, 276.31it/s] 98%|█████████▊| 4097/4192 [00:17<00:00, 276.99it/s] 99%|█████████▉| 4161/4192 [00:18<00:00, 318.13it/s]100%|██████████| 4192/4192 [00:18<00:00, 232.19it/s]
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 64
hf (pretrained=microsoft/phi-2,parallelize=True,load_in_4bit=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)
|Tasks|Version|Filter|n-shot| Metric |Value |   |Stderr|
|-----|-------|------|-----:|--------|-----:|---|-----:|
|b4bqa|Yaml   |none  |     0|acc     |0.7166|±  |0.0139|
|     |       |none  |     0|acc_norm|0.7166|±  |0.0139|

Running evaluation for model microsoft/Phi-3-medium-4k-instruct
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-06-14:07:18:42,580 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-06-14:07:18:42,580 INFO     [utils.py:161] NumExpr defaulting to 8 threads.
2024-06-14:07:18:42,811 INFO     [config.py:58] PyTorch version 2.3.0 available.
2024-06-14:07:18:50,489 INFO     [__main__.py:156] Verbosity set to INFO
2024-06-14:07:18:55,228 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-06-14:07:19:20,386 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-06-14:07:19:20,389 INFO     [__main__.py:229] Selected Tasks: ['b4bqa']
2024-06-14:07:19:20,440 WARNING  [other.py:349] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2024-06-14:07:19:21,132 WARNING  [modeling_phi3.py:62] `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2024-06-14:07:19:21,132 WARNING  [modeling_phi3.py:66] Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]Downloading shards:  17%|█▋        | 1/6 [00:21<01:46, 21.28s/it]Downloading shards:  33%|███▎      | 2/6 [00:49<01:41, 25.27s/it]Downloading shards:  50%|█████     | 3/6 [01:17<01:19, 26.51s/it]Downloading shards:  67%|██████▋   | 4/6 [01:44<00:53, 26.91s/it]Downloading shards:  83%|████████▎ | 5/6 [02:12<00:27, 27.01s/it]Downloading shards: 100%|██████████| 6/6 [02:33<00:00, 25.10s/it]Downloading shards: 100%|██████████| 6/6 [02:33<00:00, 25.57s/it]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:10<00:52, 10.50s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:20<00:41, 10.43s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:31<00:31, 10.37s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:40<00:20, 10.14s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:50<00:09,  9.92s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:57<00:00,  9.00s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:57<00:00,  9.62s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-14:07:22:54,856 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-14:07:22:54,856 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-14:07:22:54,891 INFO     [task.py:343] Building contexts for task on rank 0...
2024-06-14:07:22:54,956 INFO     [evaluator.py:314] Running loglikelihood requests
  0%|          | 0/4192 [00:00<?, ?it/s]2024-06-14:07:22:56,289 WARNING  [logging.py:329] You are not running the flash-attention implementation, expect numerical differences.
  0%|          | 1/4192 [00:08<9:33:43,  8.21s/it]  2%|▏         | 65/4192 [00:09<07:19,  9.40it/s]   3%|▎         | 129/4192 [00:10<03:38, 18.55it/s]  5%|▍         | 193/4192 [00:11<02:25, 27.46it/s]  6%|▌         | 257/4192 [00:12<01:50, 35.47it/s]  8%|▊         | 321/4192 [00:13<01:31, 42.28it/s]  9%|▉         | 385/4192 [00:14<01:19, 47.89it/s] 11%|█         | 449/4192 [00:15<01:11, 52.31it/s] 12%|█▏        | 513/4192 [00:16<01:06, 55.65it/s] 14%|█▍        | 577/4192 [00:17<01:02, 58.11it/s] 15%|█▌        | 641/4192 [00:18<00:59, 59.97it/s] 17%|█▋        | 705/4192 [00:19<00:56, 61.76it/s] 18%|█▊        | 769/4192 [00:20<00:54, 63.04it/s] 20%|█▉        | 833/4192 [00:21<00:52, 63.95it/s] 21%|██▏       | 897/4192 [00:22<00:50, 64.61it/s] 23%|██▎       | 961/4192 [00:23<00:49, 65.02it/s] 24%|██▍       | 1025/4192 [00:24<00:48, 65.40it/s] 26%|██▌       | 1089/4192 [00:25<00:47, 65.72it/s] 28%|██▊       | 1153/4192 [00:26<00:46, 65.89it/s] 29%|██▉       | 1217/4192 [00:27<00:45, 65.98it/s] 31%|███       | 1281/4192 [00:28<00:44, 66.10it/s] 32%|███▏      | 1345/4192 [00:28<00:43, 66.16it/s] 34%|███▎      | 1409/4192 [00:29<00:42, 66.19it/s] 35%|███▌      | 1473/4192 [00:30<00:41, 66.23it/s] 37%|███▋      | 1537/4192 [00:31<00:39, 66.67it/s] 38%|███▊      | 1601/4192 [00:32<00:38, 66.99it/s] 40%|███▉      | 1665/4192 [00:33<00:37, 67.21it/s] 41%|████      | 1729/4192 [00:34<00:36, 67.31it/s] 43%|████▎     | 1793/4192 [00:35<00:35, 67.46it/s] 44%|████▍     | 1857/4192 [00:36<00:34, 67.57it/s] 46%|████▌     | 1921/4192 [00:37<00:33, 67.62it/s] 47%|████▋     | 1985/4192 [00:38<00:32, 67.65it/s] 49%|████▉     | 2049/4192 [00:39<00:31, 67.78it/s] 50%|█████     | 2113/4192 [00:40<00:30, 67.89it/s] 52%|█████▏    | 2177/4192 [00:41<00:29, 67.95it/s] 53%|█████▎    | 2241/4192 [00:42<00:28, 68.01it/s] 55%|█████▍    | 2305/4192 [00:43<00:27, 68.01it/s] 57%|█████▋    | 2369/4192 [00:44<00:26, 68.02it/s] 58%|█████▊    | 2433/4192 [00:45<00:25, 68.03it/s] 60%|█████▉    | 2497/4192 [00:46<00:24, 68.01it/s] 61%|██████    | 2561/4192 [00:46<00:23, 67.99it/s] 63%|██████▎   | 2625/4192 [00:47<00:23, 67.97it/s] 64%|██████▍   | 2689/4192 [00:48<00:21, 68.85it/s] 66%|██████▌   | 2753/4192 [00:49<00:20, 69.47it/s] 67%|██████▋   | 2817/4192 [00:50<00:19, 69.91it/s] 69%|██████▊   | 2881/4192 [00:51<00:18, 70.22it/s] 70%|███████   | 2945/4192 [00:52<00:17, 70.45it/s] 72%|███████▏  | 3009/4192 [00:53<00:16, 70.63it/s] 73%|███████▎  | 3073/4192 [00:54<00:15, 70.80it/s] 75%|███████▍  | 3137/4192 [00:55<00:14, 70.88it/s] 76%|███████▋  | 3201/4192 [00:55<00:13, 70.95it/s] 78%|███████▊  | 3265/4192 [00:56<00:13, 70.97it/s] 79%|███████▉  | 3329/4192 [00:57<00:12, 71.08it/s] 81%|████████  | 3393/4192 [00:58<00:11, 71.21it/s] 82%|████████▏ | 3457/4192 [00:59<00:10, 71.27it/s] 84%|████████▍ | 3521/4192 [01:00<00:09, 71.25it/s] 86%|████████▌ | 3585/4192 [01:01<00:08, 71.24it/s] 87%|████████▋ | 3649/4192 [01:02<00:07, 71.25it/s] 89%|████████▊ | 3713/4192 [01:03<00:06, 72.61it/s] 90%|█████████ | 3777/4192 [01:03<00:05, 73.54it/s] 92%|█████████▏| 3841/4192 [01:04<00:04, 74.09it/s] 93%|█████████▎| 3905/4192 [01:05<00:03, 74.61it/s] 95%|█████████▍| 3969/4192 [01:06<00:02, 74.96it/s] 96%|█████████▌| 4033/4192 [01:07<00:02, 75.30it/s] 98%|█████████▊| 4097/4192 [01:08<00:01, 75.57it/s] 99%|█████████▉| 4161/4192 [01:08<00:00, 87.63it/s]100%|██████████| 4192/4192 [01:08<00:00, 61.06it/s]
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 64
hf (pretrained=microsoft/Phi-3-medium-4k-instruct,parallelize=True,load_in_4bit=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)
|Tasks|Version|Filter|n-shot| Metric |Value |   |Stderr|
|-----|-------|------|-----:|--------|-----:|---|-----:|
|b4bqa|Yaml   |none  |     0|acc     |0.8693|±  |0.0104|
|     |       |none  |     0|acc_norm|0.8693|±  |0.0104|

Running evaluation for model mistralai/Mistral-7B-v0.3
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-06-14:07:24:09,873 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-06-14:07:24:09,874 INFO     [utils.py:161] NumExpr defaulting to 8 threads.
2024-06-14:07:24:10,102 INFO     [config.py:58] PyTorch version 2.3.0 available.
2024-06-14:07:24:17,873 INFO     [__main__.py:156] Verbosity set to INFO
2024-06-14:07:24:22,566 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-06-14:07:24:48,061 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-06-14:07:24:48,065 INFO     [__main__.py:229] Selected Tasks: ['b4bqa']
2024-06-14:07:24:48,117 WARNING  [other.py:349] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:  33%|███▎      | 1/3 [00:19<00:39, 19.53s/it]Downloading shards:  67%|██████▋   | 2/3 [00:39<00:19, 19.84s/it]Downloading shards: 100%|██████████| 3/3 [00:57<00:00, 19.01s/it]Downloading shards: 100%|██████████| 3/3 [00:57<00:00, 19.20s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:20, 10.11s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:20<00:10, 10.23s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:29<00:00,  9.88s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:29<00:00,  9.96s/it]
2024-06-14:07:26:20,440 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-14:07:26:20,440 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-14:07:26:20,475 INFO     [task.py:343] Building contexts for task on rank 0...
2024-06-14:07:26:20,540 INFO     [evaluator.py:314] Running loglikelihood requests
  0%|          | 0/4192 [00:00<?, ?it/s]  0%|          | 1/4192 [00:04<4:39:55,  4.01s/it]  2%|▏         | 65/4192 [00:04<03:34, 19.20it/s]   3%|▎         | 129/4192 [00:05<01:48, 37.37it/s]  5%|▍         | 193/4192 [00:05<01:14, 53.87it/s]  6%|▌         | 257/4192 [00:06<00:57, 68.01it/s]  8%|▊         | 321/4192 [00:06<00:48, 80.09it/s]  9%|▉         | 385/4192 [00:07<00:42, 89.68it/s] 11%|█         | 449/4192 [00:07<00:38, 97.07it/s] 12%|█▏        | 513/4192 [00:08<00:35, 102.59it/s] 14%|█▍        | 577/4192 [00:08<00:33, 106.81it/s] 15%|█▌        | 641/4192 [00:09<00:32, 109.86it/s] 17%|█▋        | 705/4192 [00:10<00:31, 112.10it/s] 18%|█▊        | 769/4192 [00:10<00:30, 113.62it/s] 20%|█▉        | 833/4192 [00:11<00:29, 114.72it/s] 21%|██▏       | 897/4192 [00:11<00:28, 115.46it/s] 23%|██▎       | 961/4192 [00:12<00:27, 115.92it/s] 24%|██▍       | 1025/4192 [00:12<00:27, 117.15it/s] 26%|██▌       | 1089/4192 [00:13<00:26, 118.15it/s] 28%|██▊       | 1153/4192 [00:13<00:25, 118.75it/s] 29%|██▉       | 1217/4192 [00:14<00:24, 119.25it/s] 31%|███       | 1281/4192 [00:14<00:24, 119.74it/s] 32%|███▏      | 1345/4192 [00:15<00:23, 119.97it/s] 34%|███▎      | 1409/4192 [00:15<00:23, 120.07it/s] 35%|███▌      | 1473/4192 [00:16<00:22, 120.17it/s] 37%|███▋      | 1537/4192 [00:17<00:22, 120.18it/s] 38%|███▊      | 1601/4192 [00:17<00:21, 120.39it/s] 40%|███▉      | 1665/4192 [00:18<00:20, 120.58it/s] 41%|████      | 1729/4192 [00:18<00:20, 120.70it/s] 43%|████▎     | 1793/4192 [00:19<00:19, 120.74it/s] 44%|████▍     | 1857/4192 [00:19<00:19, 120.82it/s] 46%|████▌     | 1921/4192 [00:20<00:18, 120.94it/s] 47%|████▋     | 1985/4192 [00:20<00:18, 120.96it/s] 49%|████▉     | 2049/4192 [00:21<00:17, 120.94it/s] 50%|█████     | 2113/4192 [00:21<00:17, 120.92it/s] 52%|█████▏    | 2177/4192 [00:22<00:16, 122.02it/s] 53%|█████▎    | 2241/4192 [00:22<00:15, 122.88it/s] 55%|█████▍    | 2305/4192 [00:23<00:15, 123.42it/s] 57%|█████▋    | 2369/4192 [00:23<00:14, 123.92it/s] 58%|█████▊    | 2433/4192 [00:24<00:14, 124.23it/s] 60%|█████▉    | 2497/4192 [00:24<00:13, 124.62it/s] 61%|██████    | 2561/4192 [00:25<00:13, 124.85it/s] 63%|██████▎   | 2625/4192 [00:25<00:12, 125.07it/s] 64%|██████▍   | 2689/4192 [00:26<00:12, 125.23it/s] 66%|██████▌   | 2753/4192 [00:26<00:11, 125.61it/s] 67%|██████▋   | 2817/4192 [00:27<00:10, 125.85it/s] 69%|██████▊   | 2881/4192 [00:27<00:10, 126.00it/s] 70%|███████   | 2945/4192 [00:28<00:09, 126.13it/s] 72%|███████▏  | 3009/4192 [00:28<00:09, 126.19it/s] 73%|███████▎  | 3073/4192 [00:29<00:08, 126.21it/s] 75%|███████▍  | 3137/4192 [00:29<00:08, 126.20it/s] 76%|███████▋  | 3201/4192 [00:30<00:07, 126.25it/s] 78%|███████▊  | 3265/4192 [00:30<00:07, 127.30it/s] 79%|███████▉  | 3329/4192 [00:31<00:06, 128.00it/s] 81%|████████  | 3393/4192 [00:31<00:06, 128.54it/s] 82%|████████▏ | 3457/4192 [00:32<00:05, 128.90it/s] 84%|████████▍ | 3521/4192 [00:32<00:05, 129.16it/s] 86%|████████▌ | 3585/4192 [00:33<00:04, 129.43it/s] 87%|████████▋ | 3649/4192 [00:33<00:04, 129.39it/s] 89%|████████▊ | 3713/4192 [00:34<00:03, 129.39it/s] 90%|█████████ | 3777/4192 [00:34<00:03, 129.75it/s] 92%|█████████▏| 3841/4192 [00:35<00:02, 130.04it/s] 93%|█████████▎| 3905/4192 [00:35<00:02, 130.13it/s] 95%|█████████▍| 3969/4192 [00:36<00:01, 130.21it/s] 96%|█████████▌| 4033/4192 [00:36<00:01, 130.25it/s] 98%|█████████▊| 4097/4192 [00:37<00:00, 132.77it/s] 99%|█████████▉| 4161/4192 [00:37<00:00, 152.42it/s]100%|██████████| 4192/4192 [00:37<00:00, 111.46it/s]
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 64
hf (pretrained=mistralai/Mistral-7B-v0.3,parallelize=True,load_in_4bit=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)
|Tasks|Version|Filter|n-shot| Metric |Value |   |Stderr|
|-----|-------|------|-----:|--------|-----:|---|-----:|
|b4bqa|Yaml   |none  |     0|acc     |0.9103|±  |0.0088|
|     |       |none  |     0|acc_norm|0.9103|±  |0.0088|

Running evaluation for model Qwen/Qwen2-7B
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-06-14:07:27:04,241 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-06-14:07:27:04,241 INFO     [utils.py:161] NumExpr defaulting to 8 threads.
2024-06-14:07:27:04,470 INFO     [config.py:58] PyTorch version 2.3.0 available.
2024-06-14:07:27:12,181 INFO     [__main__.py:156] Verbosity set to INFO
2024-06-14:07:27:16,822 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
2024-06-14:07:27:42,201 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.
2024-06-14:07:27:42,204 INFO     [__main__.py:229] Selected Tasks: ['b4bqa']
2024-06-14:07:27:42,257 WARNING  [other.py:349] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/ch225816/miniconda3/envs/harness/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:  25%|██▌       | 1/4 [00:20<01:02, 20.87s/it]Downloading shards:  50%|█████     | 2/4 [00:46<00:47, 23.74s/it]Downloading shards:  75%|███████▌  | 3/4 [01:12<00:24, 24.88s/it]Downloading shards: 100%|██████████| 4/4 [01:35<00:00, 23.89s/it]Downloading shards: 100%|██████████| 4/4 [01:35<00:00, 23.81s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:24,  8.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:15<00:15,  7.78s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:23<00:07,  7.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.60s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-14:07:29:56,636 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-14:07:29:56,636 WARNING  [task.py:288] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-06-14:07:29:56,668 INFO     [task.py:343] Building contexts for task on rank 0...
2024-06-14:07:29:56,734 INFO     [evaluator.py:314] Running loglikelihood requests
  0%|          | 0/4192 [00:00<?, ?it/s]  0%|          | 1/4192 [00:03<4:16:23,  3.67s/it]  2%|▏         | 65/4192 [00:04<03:16, 21.02it/s]   3%|▎         | 129/4192 [00:04<01:38, 41.14it/s]  5%|▍         | 193/4192 [00:05<01:07, 59.43it/s]  6%|▌         | 257/4192 [00:05<00:52, 75.57it/s]  8%|▊         | 321/4192 [00:06<00:43, 88.91it/s]  9%|▉         | 385/4192 [00:06<00:38, 99.55it/s] 11%|█         | 449/4192 [00:07<00:34, 107.72it/s] 12%|█▏        | 513/4192 [00:07<00:32, 113.86it/s] 14%|█▍        | 577/4192 [00:08<00:30, 118.37it/s] 15%|█▌        | 641/4192 [00:08<00:29, 121.69it/s] 17%|█▋        | 705/4192 [00:09<00:28, 124.08it/s] 18%|█▊        | 769/4192 [00:09<00:27, 125.70it/s] 20%|█▉        | 833/4192 [00:10<00:26, 126.91it/s] 21%|██▏       | 897/4192 [00:10<00:25, 127.71it/s] 23%|██▎       | 961/4192 [00:11<00:25, 128.16it/s] 24%|██▍       | 1025/4192 [00:11<00:24, 129.64it/s] 26%|██▌       | 1089/4192 [00:12<00:23, 130.72it/s] 28%|██▊       | 1153/4192 [00:12<00:23, 131.50it/s] 29%|██▉       | 1217/4192 [00:13<00:22, 132.07it/s] 31%|███       | 1281/4192 [00:13<00:21, 132.44it/s] 32%|███▏      | 1345/4192 [00:13<00:21, 132.70it/s] 34%|███▎      | 1409/4192 [00:14<00:20, 133.06it/s] 35%|███▌      | 1473/4192 [00:14<00:20, 133.35it/s] 37%|███▋      | 1537/4192 [00:15<00:19, 133.81it/s] 38%|███▊      | 1601/4192 [00:15<00:19, 134.25it/s] 40%|███▉      | 1665/4192 [00:16<00:18, 134.42it/s] 41%|████      | 1729/4192 [00:16<00:18, 134.48it/s] 43%|████▎     | 1793/4192 [00:17<00:17, 134.60it/s] 44%|████▍     | 1857/4192 [00:17<00:17, 134.67it/s] 46%|████▌     | 1921/4192 [00:18<00:16, 134.64it/s] 47%|████▋     | 1985/4192 [00:18<00:16, 134.66it/s] 49%|████▉     | 2049/4192 [00:19<00:15, 134.64it/s] 50%|█████     | 2113/4192 [00:19<00:15, 136.26it/s] 52%|█████▏    | 2177/4192 [00:20<00:14, 137.38it/s] 53%|█████▎    | 2241/4192 [00:20<00:14, 138.16it/s] 55%|█████▍    | 2305/4192 [00:21<00:13, 138.72it/s] 57%|█████▋    | 2369/4192 [00:21<00:13, 139.02it/s] 58%|█████▊    | 2433/4192 [00:21<00:12, 139.22it/s] 60%|█████▉    | 2497/4192 [00:22<00:12, 139.49it/s] 61%|██████    | 2561/4192 [00:22<00:11, 139.65it/s] 63%|██████▎   | 2625/4192 [00:23<00:11, 139.79it/s] 64%|██████▍   | 2689/4192 [00:23<00:10, 139.81it/s] 66%|██████▌   | 2753/4192 [00:24<00:10, 139.83it/s] 67%|██████▋   | 2817/4192 [00:24<00:09, 139.76it/s] 69%|██████▊   | 2881/4192 [00:25<00:09, 140.07it/s] 70%|███████   | 2945/4192 [00:25<00:08, 140.27it/s] 72%|███████▏  | 3009/4192 [00:26<00:08, 140.44it/s] 73%|███████▎  | 3073/4192 [00:26<00:07, 140.44it/s] 75%|███████▍  | 3137/4192 [00:26<00:07, 140.46it/s] 76%|███████▋  | 3201/4192 [00:27<00:07, 140.48it/s] 78%|███████▊  | 3265/4192 [00:27<00:06, 140.48it/s] 79%|███████▉  | 3329/4192 [00:28<00:06, 140.43it/s] 81%|████████  | 3393/4192 [00:28<00:05, 140.40it/s] 82%|████████▏ | 3457/4192 [00:29<00:05, 140.32it/s] 84%|████████▍ | 3521/4192 [00:29<00:04, 141.84it/s] 86%|████████▌ | 3585/4192 [00:30<00:04, 143.00it/s] 87%|████████▋ | 3649/4192 [00:30<00:03, 143.74it/s] 89%|████████▊ | 3713/4192 [00:31<00:03, 144.30it/s] 90%|█████████ | 3777/4192 [00:31<00:02, 144.62it/s] 92%|█████████▏| 3841/4192 [00:31<00:02, 144.94it/s] 93%|█████████▎| 3905/4192 [00:32<00:01, 145.01it/s] 95%|█████████▍| 3969/4192 [00:32<00:01, 145.57it/s] 96%|█████████▌| 4033/4192 [00:33<00:01, 145.95it/s] 98%|█████████▊| 4097/4192 [00:33<00:00, 146.08it/s] 99%|█████████▉| 4161/4192 [00:33<00:00, 168.67it/s]100%|██████████| 4192/4192 [00:33<00:00, 123.67it/s]
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 64
hf (pretrained=Qwen/Qwen2-7B,parallelize=True,load_in_4bit=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)
|Tasks|Version|Filter|n-shot| Metric |Value |   |Stderr|
|-----|-------|------|-----:|--------|-----:|---|-----:|
|b4bqa|Yaml   |none  |     0|acc     |0.9313|±  |0.0078|
|     |       |none  |     0|acc_norm|0.9313|±  |0.0078|

